{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d53b3aa-6a64-4f6a-9f57-d96c06ff0ee0",
   "metadata": {},
   "source": [
    "# differential-expression-analysis-naive\n",
    "8.12.24\n",
    "\n",
    "Our standard differential expression analysis workflow, ensuring a 1-to-1 comparison of Lupine, Dream and naive imputation. Here we're getting DE imputed proteins after naive imputation slash no impute. \n",
    "\n",
    "Need to make sure that we're comparing the same proteins across all three of these notebooks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43326d28-db93-4ed9-925e-caf619bf4163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "from Bio import SeqIO\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb6e19a-aae5-49a0-945f-8cd00b8af387",
   "metadata": {},
   "source": [
    "#### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c87f6215-3540-44d6-94b4-97d28a23964e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The unimputed joint quants matrix\n",
    "joint_fname=\"/net/noble/vol2/home/lincolnh/data/quant-data/UMich-normalized/joint-quants-normalized-shifted.csv\"\n",
    "min_pres=18\n",
    "\n",
    "# The Ensemble (GENCODEv44) fasta\n",
    "ensembl_path=\"/net/noble/vol2/home/lincolnh/code/2023_harris_deep_impute/results/2023-11-13_UMich_dataset/fastas/\"\n",
    "ensembl_df=\"gencode.v44.pc_translations.fa\"\n",
    "\n",
    "# The HGNC database file\n",
    "hgnc_database_path=\"/net/noble/vol2/home/lincolnh/data/quant-data/HGNC_database.txt\"\n",
    "\n",
    "# The metadata dictionary, previously created\n",
    "meta_path=\"/net/noble/vol2/home/lincolnh/code/2023_harris_deep_impute/results/2024-05-10_metadata_mapping/meta-dict.csv\"\n",
    "\n",
    "cohort_ids=[\"BRCA\", \"CCRCC\", \"COAD\", \"GBM\", \"HGSC\", \n",
    "            \"HNSCC\", \"LSCC\", \"LUAD\", \"PDAC\", \"UCEC\"]\n",
    "\n",
    "# Set the thresholds\n",
    "adjusted_alpha=1e-3\n",
    "fc_thresh=0.5 \n",
    "pres_frac_thresh=0.5 # Default here is 50%\n",
    "\n",
    "curr_cohort = \"CCRCC\"\n",
    "\n",
    "# The random generator, for Gaussian sample impute\n",
    "rng = np.random.default_rng(18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa371a70-e61e-497c-b286-3fe30ad76f44",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5e295ef2-11d4-45fd-8039-b11b956c4429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_draw_impute(vec):\n",
    "    \"\"\"\n",
    "    Imputes for a single *column* at a time using the Gaussian\n",
    "    random draw procedure. This closely resembles the Perseus\n",
    "    procedure described here: \n",
    "    https://cox-labs.github.io/coxdocs/replacemissingfromgaussian.html\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vec : np.array, \n",
    "        A 1D vector, that is, column from the matrix\n",
    "        to be imputed. \n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    vec_recon : np.array, \n",
    "        The imputed vector\n",
    "    \"\"\"\n",
    "    width_param=0.3\n",
    "    downshift_param=1.8\n",
    "    #print(len(vec))\n",
    "    \n",
    "    # Get the means and std\n",
    "    v_mean = np.nanmean(vec)\n",
    "    v_std = np.nanstd(vec)\n",
    "    \n",
    "    # Get the locations of the MVs\n",
    "    nans = np.isnan(vec)\n",
    "    vec_recon = vec.copy()\n",
    "    \n",
    "    # How many total MVs? \n",
    "    n_mv = np.count_nonzero(nans)\n",
    "    \n",
    "    center = v_mean - (v_std * downshift_param)\n",
    "\n",
    "    # Replace missing values with random draws\n",
    "    vec_recon[nans] = rng.normal(\n",
    "                        loc=center,\n",
    "                        scale=v_std*width_param,\n",
    "                        size=n_mv\n",
    "    )\n",
    "    # Make sure we don't have any negative values\n",
    "    vec_recon = np.abs(vec_recon)\n",
    "    \n",
    "    return vec_recon\n",
    "\n",
    "def bh_adjustment(pvals):\n",
    "    \"\"\"\n",
    "    Performs the Benjamini-Hochberg procedure\n",
    "    for p-value ADJUSTMENT. So this means we actually \n",
    "    return a list of corrected p-values, not just a \n",
    "    boolean specifying which p-values to keep per the\n",
    "    FDR controlled at some threshold. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pvals : np.ndarray, \n",
    "        The sorted list of uncorrected p-values. Sorted\n",
    "        from smallest to largest\n",
    "\n",
    "    Returns\n",
    "    -----------\n",
    "    pvals_adjusted : list, \n",
    "        A list of the BH adjusted p-values\n",
    "    \"\"\"\n",
    "    pvals_adjusted = []\n",
    "\n",
    "    for i in range(0, len(pvals)):\n",
    "        rank = i + 1\n",
    "        curr_pval = pvals[i]\n",
    "        pval_adj = (curr_pval * len(pvals)) / rank\n",
    "        pvals_adjusted.append(pval_adj)        \n",
    "\n",
    "    return pvals_adjusted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35088f07-c311-4231-ab22-6fc2d7064669",
   "metadata": {},
   "source": [
    "#### Subset the metadata to a single cohort, get the tumor and nontumor sample IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9b2d84e0-6c0e-47f5-b4f3-e09b917f99e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n",
      "84\n"
     ]
    }
   ],
   "source": [
    "# Might need `index_col=0` here\n",
    "meta_dict = pd.read_csv(meta_path)\n",
    "meta_dict = meta_dict[meta_dict[\"cohort\"] == curr_cohort]\n",
    "meta_dict = meta_dict.reset_index(drop=True)\n",
    "\n",
    "tumor_samples_meta = meta_dict[(meta_dict[\"sample_type\"] == \"Primary Tumor\") | (meta_dict[\"sample_type\"] == \"Tumor\")]\n",
    "nontumor_samples_meta = meta_dict[(meta_dict[\"sample_type\"] != \"Primary Tumor\") & (meta_dict[\"sample_type\"] != \"Tumor\")]\n",
    "\n",
    "tumor_IDs = list(tumor_samples_meta[\"aliquot_ID\"])\n",
    "nontumor_IDs = list(nontumor_samples_meta[\"aliquot_ID\"])\n",
    "\n",
    "print(len(tumor_IDs))\n",
    "print(len(nontumor_IDs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb425c05-2c9b-426a-928a-9a79c394fd9a",
   "metadata": {},
   "source": [
    "#### Pre-process the unimputed joint quants matrix\n",
    "This should get us to the exact same starting point as the Lupine ensemble impute procedure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0aef92b5-9b2f-460f-b48d-1e3ef118a2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joint quants mat shape, post-filter: (18162, 1755)\n"
     ]
    }
   ],
   "source": [
    "# Read in the joint quants matrix\n",
    "joint_mat = pd.read_csv(joint_fname, index_col=0)\n",
    "\n",
    "# Remove some of these extraneous runs\n",
    "keywords = [\"RefInt\", \"QC\", \"pool\", \"Tumor\", \"Pooled\", \n",
    "            \"Pool\", \"Reference\", \"NCI\", \"NX\", \"Ref\"]\n",
    "to_drop = []\n",
    "\n",
    "for sample_id in list(joint_mat.columns):\n",
    "    exclude=False\n",
    "    for kw in keywords:\n",
    "        if kw in sample_id:\n",
    "            exclude=True\n",
    "            break\n",
    "    to_drop.append(exclude)\n",
    "\n",
    "keep_cols = np.array(joint_mat.columns)[~np.array(to_drop)]\n",
    "joint_mat = joint_mat[keep_cols]\n",
    "\n",
    "joint = np.array(joint_mat)\n",
    "\n",
    "# Remove proteins with too many missing values\n",
    "num_present = np.sum(~np.isnan(joint), axis=1)\n",
    "discard = num_present < min_pres\n",
    "joint = np.delete(joint, discard, axis=0)\n",
    "keep_prots = np.array(joint_mat.index)[~discard]\n",
    "\n",
    "print(f\"joint quants mat shape, post-filter: {joint.shape}\")\n",
    "\n",
    "joint_start = pd.DataFrame(joint, columns=keep_cols, index=keep_prots)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd381964-990d-4585-b1de-fe54ed803ba2",
   "metadata": {},
   "source": [
    "#### Exponentiate to get the original, untransformed intensities\n",
    "These quants had previously been log2 transformed (by the CPTAC project). So we're inversing that transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "218d8a14-8ddc-4dda-8763-ab1ab442e9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_start = np.power(2, joint_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9c1a6e-6e5f-4417-9f65-2d43319f6009",
   "metadata": {},
   "source": [
    "#### Remove the proteins with >50% missingness\n",
    "From the unimputed runs corresponding to the current cohort. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "af743c6c-d6fb-449d-b161-65dd00187e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9324, 194)\n"
     ]
    }
   ],
   "source": [
    "cohort_quants_start = joint_start[tumor_IDs + nontumor_IDs]\n",
    "\n",
    "num_present = np.sum(~np.isnan(cohort_quants_start), axis=1)\n",
    "pres_fracs = num_present / cohort_quants_start.shape[1]\n",
    "\n",
    "cohort_quants = cohort_quants_start[pres_fracs >= pres_frac_thresh]\n",
    "print(cohort_quants.shape)\n",
    "\n",
    "keep_prots_cohort = list(cohort_quants.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f675a237-c90b-4f11-9f83-b867a4089631",
   "metadata": {},
   "source": [
    "#### Get quants matrices for tumor and non-tumor samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6050909e-af1c-454a-8033-7a03c57745e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9324, 110)\n",
      "(9324, 84)\n"
     ]
    }
   ],
   "source": [
    "tumor_quants = cohort_quants[tumor_IDs]\n",
    "nontumor_quants = cohort_quants[nontumor_IDs]\n",
    "\n",
    "tumor_mat = np.array(tumor_quants)\n",
    "nontumor_mat = np.array(nontumor_quants)\n",
    "\n",
    "print(tumor_mat.shape)\n",
    "print(nontumor_mat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b789ee-b43c-4692-b07d-87ffac6e9ab0",
   "metadata": {},
   "source": [
    "#### The optional naive impute step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d3a0cb87-a057-4a8e-8372-e2dddeda88c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tumor_mat = np.apply_along_axis(random_draw_impute, 0, tumor_mat)\n",
    "nontumor_mat = np.apply_along_axis(random_draw_impute, 0, nontumor_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9721c6-4470-4eb9-ae92-897803919701",
   "metadata": {},
   "source": [
    "#### Calculate the Wilcoxon-rank sum statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5583b92a-8761-4086-b6ae-614af0f30a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals = []\n",
    "rs_stats = []\n",
    "\n",
    "for i in range(0, tumor_mat.shape[0]):\n",
    "    stat, pval = stats.ranksums(tumor_mat[i], nontumor_mat[i], nan_policy=\"omit\")\n",
    "    pvals.append(pval)\n",
    "    rs_stats.append(stat)\n",
    "\n",
    "# Init a dataframe to hold the p-values and adjusted p-values\n",
    "stats_df = pd.DataFrame(columns = [\"ENSP\", \"pval\", \"adj_pval\", \"orig_idx\"])\n",
    "stats_df[\"ENSP\"] = list(tumor_quants.index)\n",
    "stats_df[\"pval\"] = pvals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ceb9ae-52c6-4df8-8c29-c84e56397cf0",
   "metadata": {},
   "source": [
    "#### Do the Benjamini-Hochberg correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0874acf7-f7f4-4b95-a41a-d2c78395c46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by uncorrected p-values\n",
    "stats_df = stats_df.sort_values(by=\"pval\")\n",
    "# Do the BH adjustment\n",
    "pvals_corrected = bh_adjustment(np.array(stats_df[\"pval\"]))\n",
    "stats_df[\"adj_pval\"] = pvals_corrected\n",
    "stats_df[\"orig_idx\"] = list(stats_df.index)\n",
    "\n",
    "# Return to the initial order\n",
    "stats_df = stats_df.sort_values(by=\"orig_idx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6257fe-6c73-4898-8705-54350ebee58f",
   "metadata": {},
   "source": [
    "#### Get the log fold changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "54aa397f-5f84-4f89-8356-c3af1ebe35dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tumor_expr_means = np.nanmean(tumor_mat, axis=1)\n",
    "nontumor_expr_means = np.nanmean(nontumor_mat, axis=1)\n",
    "\n",
    "log_fold_changes = np.log2(tumor_expr_means / nontumor_expr_means)\n",
    "\n",
    "fdr = -np.log10(pvals_corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21091bb-4267-413b-8616-0f85317b796b",
   "metadata": {},
   "source": [
    "#### Create an aggregated dataframe for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4455c207-7261-453a-827b-5b222c0212e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated = pd.DataFrame(columns=[\"ENSP\", \"HGNC\", \"p-value\", \"FC\"])\n",
    "aggregated[\"ENSP\"] = list(tumor_quants.index)\n",
    "aggregated[\"p-value\"] = pvals_corrected\n",
    "aggregated[\"FC\"] = log_fold_changes\n",
    "aggregated[\"FDR\"] = fdr\n",
    "#aggregated.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556ed9e2-66d8-4574-a93d-50c0fc6dcbea",
   "metadata": {},
   "source": [
    "#### Create a dictionary mapping ENSPs to HGNCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "99748453-3e1a-4d93-9cb3-a435db82209e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1932284/1917651185.py:2: DtypeWarning: Columns (31,38) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  hgnc_db = pd.read_csv(hgnc_database_path, sep=\"\\t\")\n"
     ]
    }
   ],
   "source": [
    "# Read in the HGNC database file\n",
    "hgnc_db = pd.read_csv(hgnc_database_path, sep=\"\\t\")\n",
    "\n",
    "# Read in the ENSEMBL fasta\n",
    "ensembl_fasta = ensembl_path + ensembl_df\n",
    "fasta_seqs = SeqIO.parse(open(ensembl_fasta), \"fasta\")\n",
    "\n",
    "# Init both dictionaries\n",
    "gene_x_prot = {}\n",
    "prot_x_gene = {}\n",
    "\n",
    "# Fill in the dictionary \n",
    "for fasta in fasta_seqs:\n",
    "    name, descript, sequence = \\\n",
    "        fasta.id, fasta.description, str(fasta.seq)\n",
    "    # Get the ENSP and ENSG IDs\n",
    "    ensp_id = name.split(\"|\")[0]\n",
    "    ensg_id = name.split(\"|\")[2]\n",
    "    # Strip the \".x\" characters. Hope this is ok.\n",
    "    ensp_id = ensp_id.split(\".\")[0]\n",
    "    ensg_id = ensg_id.split(\".\")[0]\n",
    "    \n",
    "    # Update the first dictionary\n",
    "    prot_x_gene[ensp_id] = ensg_id\n",
    "    \n",
    "    # Update the second\n",
    "    if ensg_id in gene_x_prot:\n",
    "        gene_x_prot[ensg_id].append(ensp_id)\n",
    "    else:\n",
    "        gene_x_prot[ensg_id] = [ensp_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35b4474-ed8c-46c1-8b87-d47eb95403fe",
   "metadata": {},
   "source": [
    "#### Append the HGNC IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "09ac46d0-ade4-4d63-9ede-0f1e8abe1da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(0, aggregated.shape[0]):\n",
    "    curr = aggregated.iloc[idx]\n",
    "    curr_ensp = curr[\"ENSP\"]\n",
    "    try:\n",
    "        curr_ensg = prot_x_gene[curr_ensp]\n",
    "    except KeyError:\n",
    "        curr_ensg = None\n",
    "\n",
    "    # Add the ENSG ID\n",
    "    aggregated.loc[idx, \"ENSG\"] = curr_ensg\n",
    "\n",
    "    # Add in the HGNC gene ID as well \n",
    "    if curr_ensg is not None:\n",
    "        try:\n",
    "            hgnc_row = hgnc_db[hgnc_db[\"ensembl_gene_id\"] == curr_ensg]\n",
    "            hgnc_id = hgnc_row[\"symbol\"].item()\n",
    "\n",
    "            aggregated.loc[idx, \"HGNC\"] = hgnc_id\n",
    "        except ValueError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8b4ed9-c54a-4d57-a1a5-e26eeab1290d",
   "metadata": {},
   "source": [
    "#### Define up- and down-regulated genes/proteins \n",
    "According to our adjusted p-value threshold and log FC threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b7f990a3-bc97-4c3c-bb57-0d0f94fce9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num up-reg: 56\n",
      "num down-reg: 296\n"
     ]
    }
   ],
   "source": [
    "up_df = aggregated[(aggregated[\"p-value\"] < adjusted_alpha) & (aggregated[\"FC\"] >= fc_thresh)]\n",
    "down_df = aggregated[(aggregated[\"p-value\"] < adjusted_alpha) & (aggregated[\"FC\"] <= -fc_thresh)]\n",
    "\n",
    "print(f\"num up-reg: {up_df.shape[0]}\")\n",
    "print(f\"num down-reg: {down_df.shape[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-impute",
   "language": "python",
   "name": "deep-impute"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
